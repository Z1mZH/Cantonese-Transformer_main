{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01f23638",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T05:39:17.510792800Z",
     "start_time": "2025-01-12T05:39:17.045851Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([   0, 9178,   32,    2]), tensor([1, 1, 1, 1]), '<s>how are</s>')"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "from util import TokenizerUtil\n",
    "\n",
    "tokenizer = TokenizerUtil()\n",
    "\n",
    "input_ids, attention_mask = tokenizer.encode('how are you', max_length=4)\n",
    "\n",
    "input_ids, attention_mask, tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85e428b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T05:58:44.035299300Z",
     "start_time": "2025-01-12T05:58:38.789220100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ae4b90f8cbd648e3b678331b44007930"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/6575 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "467627a855f2440c87108b93e133a625"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "(1643,\n {'input_ids': tensor([[    0, 33837,    35,  ...,     1,     1,     1],\n          [    0, 33837,    35,  ...,     1,     1,     1],\n          [    0, 33837,    35,  ...,     1,     1,     1],\n          ...,\n          [    0, 33837,    35,  ...,     1,     1,     1],\n          [    0, 33837,    35,  ...,     1,     1,     1],\n          [    0, 33837,    35,  ...,     1,     1,     1]]),\n  'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n          [1, 1, 1,  ..., 0, 0, 0],\n          [1, 1, 1,  ..., 0, 0, 0],\n          ...,\n          [1, 1, 1,  ..., 0, 0, 0],\n          [1, 1, 1,  ..., 0, 0, 0],\n          [1, 1, 1,  ..., 0, 0, 0]])})"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('json', data_files='D:/Project/Pycharm project/ZZH/DL/Simple_RLHF-main/dataset/train_new.json', split='train')\n",
    "\n",
    "#2,4,4切分,取第1部分\n",
    "dataset = dataset.select(range(2200, 8775))\n",
    "\n",
    "\n",
    "def f(data):\n",
    "    #区分两种生成结果\n",
    "    chosen = data['prompt'] + data['chosen'].swapcase()\n",
    "    rejected = data['prompt'] + data['chosen']\n",
    "\n",
    "    chosen_input_ids, chosen_attention_mask = tokenizer.encode(chosen)\n",
    "    rejected_input_ids, rejected_attention_mask = tokenizer.encode(rejected)\n",
    "\n",
    "    return {\n",
    "        'chosen_input_ids': chosen_input_ids,\n",
    "        'chosen_attention_mask': chosen_attention_mask,\n",
    "        'rejected_input_ids': rejected_input_ids,\n",
    "        'rejected_attention_mask': rejected_attention_mask\n",
    "    }\n",
    "\n",
    "\n",
    "dataset = dataset.map(f)\n",
    "dataset.set_format('torch')\n",
    "\n",
    "\n",
    "def f(data):\n",
    "    chosen_input_ids = [i['chosen_input_ids'] for i in data]\n",
    "    chosen_attention_mask = [i['chosen_attention_mask'] for i in data]\n",
    "    rejected_input_ids = [i['rejected_input_ids'] for i in data]\n",
    "    rejected_attention_mask = [i['rejected_attention_mask'] for i in data]\n",
    "\n",
    "    input_ids = torch.stack(chosen_input_ids + rejected_input_ids, dim=0)\n",
    "    attention_mask = torch.stack(chosen_attention_mask +\n",
    "                                 rejected_attention_mask,\n",
    "                                 dim=0)\n",
    "\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset,\n",
    "                                     collate_fn=f,\n",
    "                                     batch_size=4,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n",
    "\n",
    "len(loader), next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dfa4c50a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T05:58:48.037321400Z",
     "start_time": "2025-01-12T05:58:46.738739800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count_require': 3.31196928, 'count_all': 3.31196928, 'ratio': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from lora import count_params\n",
    "\n",
    "\n",
    "class CriticModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        from transformers import AutoModel\n",
    "        self.rwtransformer = AutoModel.from_pretrained('facebook/opt-350m',\n",
    "                                                       dropout=0.0)\n",
    "\n",
    "        self.v_head = torch.nn.Linear(512, 1, bias=False)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        value = self.rwtransformer(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask).last_hidden_state\n",
    "        value = self.v_head(value).squeeze(-1)\n",
    "\n",
    "        loss_sum = 0.0\n",
    "        value_chosen_sum = 0.0\n",
    "        value_rejected_sum = 0.0\n",
    "        for input_ids_chosen, input_ids_rejected, value_chosen, value_rejected in zip(\n",
    "                input_ids[:4], input_ids[4:], value[:4], value[4:]):\n",
    "\n",
    "            #找出每条回答中的起止索引\n",
    "            start = (\n",
    "                input_ids_chosen == input_ids_rejected).tolist().index(False)\n",
    "\n",
    "            end_chosen = input_ids_chosen.tolist().index(\n",
    "                tokenizer.eos_token_id) + 1\n",
    "            end_rejected = input_ids_rejected.tolist().index(\n",
    "                tokenizer.eos_token_id) + 1\n",
    "            end = max(end_chosen, end_rejected)\n",
    "\n",
    "            value_chosen = value_chosen[start:end]\n",
    "            value_rejected = value_rejected[start:end]\n",
    "\n",
    "            loss = value_chosen - value_rejected\n",
    "            loss = -torch.nn.functional.logsigmoid(loss).mean()\n",
    "\n",
    "            loss_sum += loss\n",
    "            value_chosen_sum += value_chosen.mean().item()\n",
    "            value_rejected_sum += value_rejected.mean().item()\n",
    "\n",
    "        return loss_sum / 4, value_chosen_sum, value_rejected_sum\n",
    "\n",
    "\n",
    "model_critic = CriticModel()\n",
    "\n",
    "count_params(model_critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adb14bab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T05:58:49.805609800Z",
     "start_time": "2025-01-12T05:58:49.237869300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "CriticModel(\n  (rwtransformer): OPTModel(\n    (decoder): OPTDecoder(\n      (embed_tokens): Embedding(50272, 512, padding_idx=1)\n      (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n      (project_out): Linear(in_features=1024, out_features=512, bias=False)\n      (project_in): Linear(in_features=512, out_features=1024, bias=False)\n      (layers): ModuleList(\n        (0-23): 24 x OPTDecoderLayer(\n          (self_attn): OPTSdpaAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (v_head): Linear(in_features=512, out_features=1, bias=False)\n)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "from transformers import get_scheduler\n",
    "from accelerate import Accelerator\n",
    "\n",
    "\n",
    "def f():\n",
    "    params_decay = []\n",
    "    params = []\n",
    "    for name, param in model_critic.named_parameters():\n",
    "        if 'bias' in name or 'norm.weight' in name:\n",
    "            params.append(param)\n",
    "            continue\n",
    "        params_decay.append(param)\n",
    "\n",
    "    return [{\n",
    "        'params': params_decay,\n",
    "        'weight_decay': 0.1\n",
    "    }, {\n",
    "        'params': params,\n",
    "        'weight_decay': 0.0\n",
    "    }]\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(f(), lr=5e-5, betas=(0.9, 0.95))\n",
    "\n",
    "scheduler = get_scheduler(name='cosine',\n",
    "                          optimizer=optimizer,\n",
    "                          num_warmup_steps=0,\n",
    "                          num_training_steps=500)\n",
    "\n",
    "accelerator = Accelerator(gradient_accumulation_steps=16,\n",
    "                          mixed_precision='fp16')\n",
    "\n",
    "model_critic, loader, optimizer, scheduler = accelerator.prepare(\n",
    "    model_critic, loader, optimizer, scheduler)\n",
    "\n",
    "model_critic.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7838daa6",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T05:59:09.663844800Z",
     "start_time": "2025-01-12T05:59:07.311728200Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "False is not in list",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[22], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, data \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(loader):\n\u001B[0;32m      2\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m accelerator\u001B[38;5;241m.\u001B[39maccumulate(model_critic):\n\u001B[1;32m----> 3\u001B[0m         loss, value_chosen_sum, value_rejected_sum \u001B[38;5;241m=\u001B[39m \u001B[43mmodel_critic\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m         accelerator\u001B[38;5;241m.\u001B[39mbackward(loss)\n\u001B[0;32m      6\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m accelerator\u001B[38;5;241m.\u001B[39msync_gradients:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\utils\\operations.py:820\u001B[0m, in \u001B[0;36mconvert_outputs_to_fp32.<locals>.forward\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    819\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m--> 820\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\utils\\operations.py:808\u001B[0m, in \u001B[0;36mConvertOutputsToFp32.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    807\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m--> 808\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m convert_to_fp32(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:43\u001B[0m, in \u001B[0;36mautocast_decorator.<locals>.decorate_autocast\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     40\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_autocast\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m     42\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m autocast_instance:\n\u001B[1;32m---> 43\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[19], line 29\u001B[0m, in \u001B[0;36mCriticModel.forward\u001B[1;34m(self, input_ids, attention_mask)\u001B[0m\n\u001B[0;32m     23\u001B[0m value_rejected_sum \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m input_ids_chosen, input_ids_rejected, value_chosen, value_rejected \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(\n\u001B[0;32m     25\u001B[0m         input_ids[:\u001B[38;5;241m4\u001B[39m], input_ids[\u001B[38;5;241m4\u001B[39m:], value[:\u001B[38;5;241m4\u001B[39m], value[\u001B[38;5;241m4\u001B[39m:]):\n\u001B[0;32m     26\u001B[0m \n\u001B[0;32m     27\u001B[0m     \u001B[38;5;66;03m#找出每条回答中的起止索引\u001B[39;00m\n\u001B[0;32m     28\u001B[0m     start \u001B[38;5;241m=\u001B[39m \u001B[43m(\u001B[49m\n\u001B[1;32m---> 29\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_ids_chosen\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43minput_ids_rejected\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtolist\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     31\u001B[0m     end_chosen \u001B[38;5;241m=\u001B[39m input_ids_chosen\u001B[38;5;241m.\u001B[39mtolist()\u001B[38;5;241m.\u001B[39mindex(\n\u001B[0;32m     32\u001B[0m         tokenizer\u001B[38;5;241m.\u001B[39meos_token_id) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     33\u001B[0m     end_rejected \u001B[38;5;241m=\u001B[39m input_ids_rejected\u001B[38;5;241m.\u001B[39mtolist()\u001B[38;5;241m.\u001B[39mindex(\n\u001B[0;32m     34\u001B[0m         tokenizer\u001B[38;5;241m.\u001B[39meos_token_id) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "\u001B[1;31mValueError\u001B[0m: False is not in list"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(loader):\n",
    "    with accelerator.accumulate(model_critic):\n",
    "        loss, value_chosen_sum, value_rejected_sum = model_critic(**data)\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        if accelerator.sync_gradients:\n",
    "            accelerator.clip_grad_norm_(model_critic.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    if (i + 1) % 100 == 0:\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        print(i, len(loader), loss.item(), lr, value_chosen_sum,\n",
    "              value_rejected_sum)\n",
    "\n",
    "    if i == 2000:\n",
    "        break\n",
    "\n",
    "torch.save(model_critic.to('cpu'), 'model/critic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
